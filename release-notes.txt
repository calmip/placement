v 1.14.2:
---------
    BUG: Sometimes old step_ directories remain on the node, belonging to finished jobs
         This induced some trouble in the cpusets display

v 1.14.1:
---------
    BUG: in the PYTHONPATH definition
    
v 1.14.0:
---------
    Now displaying job info, taken from the cpuset

v 1.13.1:
---------
    BUG - sometimes hang when displaying threads

v 1.13.0
--------
    Threads mode: coloring processes and threads from the jobid (works with slurm)

v 1.12.1
--------
    Call script: Introduced the environment variable PYTHONIOENCODING to force the encoding, whatever the locale selected on the host

v 1.12.0
--------
    Introduced --intel_pin_domain (-z) switch: to export an environment variable for explicit placement with intel mpi's mpiexec.hydra

v 1.11.1
--------
    Bug correction: placement did crash when ckecing gpus with more than 100% activity !

v 1.11.0
--------
    In the threads representation with gpus, processes belonging to other users are more clearer depicted
    We use a . instead of a tag to represent them. This is used for nvidia-mps processes, OR for processes from other users (shared nodes)
    It is now possible checking processes on a node even when no process is running. This is useful with the gpus,
    as sometimes the process does not take any resource from the cpu, but runs on the gpu
    In the threads representation with --memory switch, we now display the quantity of memory allocated on the node, not only the repartition
    
v 1.10.0
--------
    In the threads representation (--check, --host etc), we now display the Session Id
    Processes are sorted using the session id as key
    This is very useful for shared nodes

v 1.9.0
-------
    Support for shared nodes is back
   
v 1.8.0
-------
    External binaries are called through ssh on remote nodes, this is required 
    by some supercomputers
    Bugguy support for share nodes removed    

v 1.7.0
-------
    New cores addressing supported (encountered on Dell servers for instance)
    placement --make_mpi_aware is only useful with slurm, on a SHARED node
    So you can use directly --mpi_aware
    
v 1.6.0
-------
    Now need python3 (3.5.x min)
    Error messages now on stderr, no more stdout -> better log files in slurm.out
    --check = May display up to 296 tasks, thanks to utf8 outputs !
    GPU = display processes known by the gpus and memory used

v 1.5.0
-------
    Introducing the --summary switch (still experimental)
    --no_ansi may be useful with --summary
    Redesigned the --check output

v 1.4.0
-------
    Support for the GPU
    With --checkme: we display only the first node, even if we have a lot of reserved nodes

v 1.3.0
-------
    Trying to guess architecture from slurm.conf
    Added mpi_aware mode

v 1.2.0
-------
    --intel_affinity and --gnu_affinity switches
    --memory with --check to display memory consumption of the processes 
    --Better display with --check
    
v 1.1.0
------
    Refactoring for a better code
    Unit tests added to the git repository
    Added switches --check and --checkme
    Added switches --srun and --numactl
